{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/nainishdhanorkar/Downloads/task/macbook-air-13inch-m4-2025-info (1).pdf\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=file_path)\n",
    "documents = loader.load()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,\n",
       " 'Before using MacBook Air, review the MacBook Air  \\nGetting Started Guide  at support.apple.com/guide/\\nmacbook-air. Retain documentation for future \\nreference.\\nSafety and Handling\\nSee â€œSafety, handling')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=120,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "len(splits), splits[0].page_content[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a concise assistant. Use the context to answer.\\n\"\n",
    "    \"If the answer is not in the context, say you don't know.\\n\\n\"\n",
    "    \"Context:\\n{context}\\n\\n\"\n",
    "    \"Question: {question}\"\n",
    ")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product is the MacBook Air, as indicated in the title of the document. It includes features related to energy efficiency, compliance with ENERGY STAR guidelines, and disposal and recycling information. The device is designed to promote energy-efficient use and is shipped with power management enabled, causing it to sleep after 10 minutes of inactivity. It is important to dispose of the product and its battery separately from household waste at designated collection points to conserve resources and protect the environment. Additionally, the built-in battery should only be replaced or repaired by trained technicians to avoid risks such as overheating or fire.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"give me some info of product\"\n",
    "response = rag_chain.invoke(question)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langgraph.types import interrupt, Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "\n",
    "     messages: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "\n",
    "    decision = interrupt({\n",
    "        \"type\": \"approval\",\n",
    "        \"reason\": \"Model is about to answer a user question.\",\n",
    "        \"question\": state[\"messages\"],\n",
    "        \"instruction\": \"Approve this question? yes/no\"\n",
    "    })\n",
    "    \n",
    "    if decision[\"approved\"] == 'no':\n",
    "        return {\"messages\": \"Not approved.\"}\n",
    "\n",
    "    else:\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage, AIMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the graph: START -> chat -> END\n",
    "builder = StateGraph(ChatState)\n",
    "\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "\n",
    "builder.add_edge(START, \"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "# Checkpointer is required for interrupts\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the app\n",
    "app = builder.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new thread id for this conversation\n",
    "config = {\"configurable\": {\"thread_id\": '1234'}}\n",
    "\n",
    "# ---- STEP 1: user asks a question ----\n",
    "initial_input = {\n",
    "    \"messages\":\"Explain gradient descent in very simple terms.\"\n",
    "}\n",
    "\n",
    "# Invoke the graph for the first time\n",
    "result = app.invoke(initial_input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'Explain gradient descent in very simple terms.',\n",
       " '__interrupt__': [Interrupt(value={'type': 'approval', 'reason': 'Model is about to answer a user question.', 'question': 'Explain gradient descent in very simple terms.', 'instruction': 'Approve this question? yes/no'}, id='3f3904530a39f74eb775f20b0d1fc64e')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'approval',\n",
       " 'reason': 'Model is about to answer a user question.',\n",
       " 'question': 'Explain gradient descent in very simple terms.',\n",
       " 'instruction': 'Approve this question? yes/no'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = result['__interrupt__'][0].value\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(f\"\\nBackend message - {message} \\n Approve this question? (y/n): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume the graph with the approval decision\n",
    "final_result = app.invoke(\n",
    "    Command(resume={\"approved\": user_input}),\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': 'Not approved.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ROUTER_PROMPT = \"\"\"\n",
    "You are a Query Router Agent.\n",
    "Your task is to read the user's query and reply with only one agent name based on the intent.\n",
    "\n",
    "Rules:\n",
    "1. If the user query is about math operations (calculation, equation, arithmetic, numbers) â†’ reply \"math\"\n",
    "2. If the user query is about getting information, explanation, facts, or knowledge â†’ reply \"knowledge\"\n",
    "3. If the user wants to book a ground / playground / turf / ground reservation â†’ reply \"ground\"\n",
    "4. For any other request, reply exactly: \"out of my known\"\n",
    "\n",
    "Strict instructions:\n",
    "- Reply with only one word.\n",
    "- Do not answer the user query.\n",
    "- Do not add punctuation or extra text.\n",
    "- Allowed outputs only: math, knowledge, ground, out of my known\n",
    "\n",
    "your query :-{query}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "from operator import add\n",
    "from langchain_core.tools import tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class BookingSchema(TypedDict):\n",
    "    user_id: str\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "    date: str\n",
    "    status:str\n",
    "\n",
    "CSV_FILE = \"bookings.csv\"\n",
    "\n",
    "\n",
    "def get_booking(user_id: str) -> List[BookingSchema]:\n",
    "    \"\"\"\n",
    "    Get all bookings for a specific user_id.\n",
    "    \n",
    "    Args:\n",
    "        user_id: The user ID to filter bookings by\n",
    "        \n",
    "    Returns:\n",
    "        List of booking dictionaries matching the user_id\n",
    "    \"\"\"\n",
    "    bookings = read_bookings_from_csv()\n",
    "    return [booking for booking in bookings if booking[\"user_id\"] == user_id]\n",
    "\n",
    "\n",
    "def is_time_slot_available(booking: BookingSchema) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a time slot is available (not conflicting with existing bookings).\n",
    "    \n",
    "    Args:\n",
    "        booking: A booking dictionary with user_id, start_time, end_time, and date\n",
    "        \n",
    "    Returns:\n",
    "        True if the time slot is available, False if it conflicts with existing bookings\n",
    "    \"\"\"\n",
    "    existing_bookings = read_bookings_from_csv()\n",
    "    \n",
    "    # Parse the new booking times\n",
    "    new_start = datetime.strptime(booking[\"start_time\"], \"%H:%M\").time()\n",
    "    new_end = datetime.strptime(booking[\"end_time\"], \"%H:%M\").time()\n",
    "    new_date = booking[\"date\"]\n",
    "    \n",
    "    for existing_booking in existing_bookings:\n",
    "        # Check if it's the same date\n",
    "        if existing_booking[\"date\"] == new_date:\n",
    "            # Parse existing booking times\n",
    "            existing_start = datetime.strptime(existing_booking[\"start_time\"], \"%H:%M\").time()\n",
    "            existing_end = datetime.strptime(existing_booking[\"end_time\"], \"%H:%M\").time()\n",
    "            \n",
    "            # Check for time overlap\n",
    "            # Two time slots overlap if:\n",
    "            # - new_start < existing_end AND new_end > existing_start\n",
    "            if new_start < existing_end and new_end > existing_start:\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def save_booking_to_csv(booking: BookingSchema) -> str:\n",
    "    \"\"\"\n",
    "    Save a booking to the CSV file.\n",
    "    If the file doesn't exist, it will be created with headers.\n",
    "    Checks for time slot conflicts before saving.\n",
    "    \n",
    "    Args:\n",
    "        booking: A booking dictionary with user_id, start_time, end_time, and date\n",
    "        \n",
    "    Returns:\n",
    "        Success message if booking is saved, or error message if time slot is occupied\n",
    "    \"\"\"\n",
    "    required_fields = [\"user_id\", \"start_time\", \"end_time\", \"date\"]\n",
    "\n",
    "    def is_complete(state: dict) -> bool:\n",
    "        return all(\n",
    "            field in state and state[field] not in (\"\", None)\n",
    "            for field in required_fields\n",
    "        )\n",
    "    if is_complete(state=booking):\n",
    "        print(\"inside the loop\")\n",
    "        # Check if time slot is available\n",
    "        if not is_time_slot_available(booking):\n",
    "            \n",
    "            booking[\"status\"]=\"Unable to book on this time. Please update your start time or end time.\"\n",
    "            return booking\n",
    "        file_exists = os.path.isfile(CSV_FILE)\n",
    "        \n",
    "        with open(CSV_FILE, 'a', newline='', encoding='utf-8') as csvfile:\n",
    "            fieldnames = ['user_id', 'start_time', 'end_time', 'date']\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            \n",
    "            # Write header if file is new\n",
    "            if not file_exists:\n",
    "                writer.writeheader()\n",
    "            \n",
    "            # Write the booking data\n",
    "            writer.writerow(booking)\n",
    "            booking[\"status\"]=f\"your booking of ground done for {booking}\"\n",
    "    \n",
    "    return booking\n",
    "\n",
    "\n",
    "def read_bookings_from_csv() -> List[BookingSchema]:\n",
    "    \"\"\"\n",
    "    Read all bookings from the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        List of all booking dictionaries from the CSV file\n",
    "    \"\"\"\n",
    "    bookings = []\n",
    "    \n",
    "    if not os.path.isfile(CSV_FILE):\n",
    "        return bookings\n",
    "    \n",
    "    with open(CSV_FILE, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            booking: BookingSchema = {\n",
    "                \"user_id\": row[\"user_id\"],\n",
    "                \"start_time\": row[\"start_time\"],\n",
    "                \"end_time\": row[\"end_time\"],\n",
    "                \"date\": row[\"date\"]\n",
    "            }\n",
    "            bookings.append(booking)\n",
    "    \n",
    "    return bookings\n",
    "\n",
    "ground_booking_bilder=StateGraph(BookingSchema)\n",
    "ground_booking_bilder.add_node(\"save_booking_to_csv\",save_booking_to_csv)\n",
    "ground_booking_bilder.add_edge(START,\"save_booking_to_csv\")\n",
    "ground_booking_bilder.add_edge(\"save_booking_to_csv\",END)\n",
    "checkpointer = MemorySaver()\n",
    "ground_book_graph=ground_booking_bilder.compile(checkpointer=checkpointer)\n",
    "config = {\"configurable\": {\"thread_id\": '12345'}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_state={\"status\":\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# -----------------------------\n",
    "# BookingSchema definition\n",
    "# -----------------------------\n",
    "class BookingSchema(TypedDict, total=False):\n",
    "    user_id: str\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "    date: str\n",
    "    status: str\n",
    "\n",
    "# -----------------------------\n",
    "# OpenAI API key setup\n",
    "# -----------------------------\n",
    "OPENAI_API_KEY=\"\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt template\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE = PROMPT_TEMPLATE = \"\"\"\n",
    "You are a booking assistant.\n",
    "\n",
    "Extract booking information from the user query.\n",
    "\n",
    "Return ONLY a valid JSON object matching:\n",
    "BookingSchema(TypedDict, total=False)\n",
    "Allowed fields: user_id, start_time, end_time, date, status\n",
    "\n",
    "STRICT RULES:\n",
    "1. Output MUST be valid JSON only. No explanation.\n",
    "2. Time format MUST be 24-hour HH:MM.\n",
    "3. Convert AM/PM to 24-hour time.\n",
    "4. Date format MUST be YYYY-MM-DD.\n",
    "5. Include ONLY fields explicitly mentioned or clearly inferred.\n",
    "6. If a field exists in the previous state and is NOT updated by the user,\n",
    "   KEEP the previous value.\n",
    "7. ðŸš« If end_time is NOT mentioned, DO NOT include end_time.\n",
    "8. ðŸš« Never copy start_time into end_time.\n",
    "9. Do NOT guess missing fields.\n",
    "10. Do NOT include empty, null, or invalid values.\n",
    "\n",
    "Previous state:\n",
    "{previous_state}\n",
    "\n",
    "User query:\n",
    "\"{user_query}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Function to update single booking state\n",
    "# -----------------------------\n",
    "def update_booking_state(user_query: str, previous_state: BookingSchema) -> BookingSchema:\n",
    "    prev_state_str = json.dumps(previous_state)\n",
    "    prompt = PROMPT_TEMPLATE.format(user_query=user_query, previous_state=prev_state_str)\n",
    "\n",
    "    # New syntax for v1.0+ OpenAI Python SDK\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful booking assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "       \n",
    "    )\n",
    "\n",
    "    ai_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        updated_state = json.loads(ai_text)\n",
    "        \n",
    "        return updated_state\n",
    "    except json.JSONDecodeError:\n",
    "        return previous_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def math_tool(first_num: float, second_num: float, operation: str) -> dict:\n",
    "    \"\"\"\n",
    "    Perform a basic arithmetic operation on two numbers.\n",
    "    Supported operations: add, sub, mul, div\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if operation == \"add\":\n",
    "            result = first_num + second_num\n",
    "        elif operation == \"sub\":\n",
    "            result = first_num - second_num\n",
    "        elif operation == \"mul\":\n",
    "            result = first_num * second_num\n",
    "        elif operation == \"div\":\n",
    "            if second_num == 0:\n",
    "                return {\"error\": \"Division by zero is not allowed\"}\n",
    "            result = first_num / second_num\n",
    "        else:\n",
    "            return {\"error\": f\"Unsupported operation '{operation}'\"}\n",
    "        \n",
    "        return {\"first_num\": first_num, \"second_num\": second_num, \"operation\": operation, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "class InitailStateState(TypedDict):\n",
    "    query: str\n",
    "    responce:str\n",
    "    \n",
    "def inital_chat(state:InitailStateState):\n",
    "  \n",
    "    query=state[\"query\"]\n",
    "    QUERY_ROUTER_PROMPT =f\"\"\"\n",
    "    You are a Query Router Agent.\n",
    "    Your task is to read the user's query and reply with only one agent name based on the intent.\n",
    "\n",
    "    Rules:\n",
    "    1. If the user query is about math operations (calculation, equation, arithmetic, numbers) â†’ reply \"math\"\n",
    "    2. If the user query is about getting information, explanation, facts, or knowledge â†’ reply \"knowledge\"\n",
    "    3. If the user wants to book a ground / playground / turf / ground reservation â†’ reply \"ground\"\n",
    "    4. For any other request, reply exactly: \"out_of_my_known\"\n",
    "\n",
    "    Strict instructions:\n",
    "    - Reply with only one word.\n",
    "    - Do not answer the user query.\n",
    "    - Do not add punctuation or extra text.\n",
    "    - Allowed outputs only: math, knowledge, ground, out_of_my_known\n",
    "\n",
    "    your query :- {query}\n",
    "    \"\"\"\n",
    "    route=llm.invoke(QUERY_ROUTER_PROMPT).content.strip()\n",
    "\n",
    "    return route\n",
    "llm_with_tools = llm.bind_tools([math_tool])\n",
    "# #dic=1\n",
    "# def ground_book(state: InitailStateState):\n",
    "    \n",
    "#     if len(dic)!=5:\n",
    "#         dic.append(\"ff\")\n",
    "#         decision = interrupt({\n",
    "#             \"type\": \"responce\",\n",
    "#             \"question\": \"my ground book quation\",\n",
    "#             \"instruction\": \"Approve this question? yes/no\"\n",
    "#         })\n",
    "#     print(\"ds\")\n",
    "#     return state\n",
    "\n",
    "def math(state:InitailStateState):\n",
    "\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    llm_with_tools = llm.bind_tools([math_tool])\n",
    "    ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "    # Tool calling case\n",
    "    if ai_msg.tool_calls:\n",
    "        tool_call = ai_msg.tool_calls[0]\n",
    "\n",
    "        tool_result = math_tool.invoke(tool_call[\"args\"])\n",
    "\n",
    "        return {\n",
    "            \"responce\": str(tool_result[\"result\"])\n",
    "        }\n",
    "\n",
    "    # Fallback\n",
    "    return {\n",
    "        \"responce\": ai_msg.content\n",
    "    }\n",
    "\n",
    "def knowledge(state:InitailStateState):\n",
    "    query=state[\"query\"]\n",
    "    # Make the LLM tool-aware\n",
    "    state[\"responce\"]=\"dsa\"\n",
    "    return state\n",
    "\n",
    "def out_of_my_known(state:InitailStateState):\n",
    "    state[\"responce\"]=\"out of know\"\n",
    "    return state\n",
    "\n",
    "\n",
    "privious_state={\n",
    "    \"user_id\": \"\",\n",
    "    \"start_time\": \"\",    # 9:00 AM in 24-hour format\n",
    "    \"end_time\": \"\",       # 5:30 PM in 24-hour format\n",
    "    \"date\": \"\"\n",
    "}\n",
    "def ground(state:InitailStateState):\n",
    "    query=state[\"query\"]\n",
    "    privious_state[\"user_id\"]=123\n",
    "    st=update_booking_state(query,privious_state)\n",
    "\n",
    "    kt=ground_book_graph.invoke(st,config=config)\n",
    "    \n",
    "    privious_state[\"date\"]=kt.get(\"date\")\n",
    "    privious_state[\"start_time\"]=kt.get(\"start_time\")\n",
    "    privious_state[\"end_time\"]=kt.get(\"end_time\")\n",
    "    \n",
    "    if not kt.get(\"status\"):\n",
    "        \n",
    "        human_answer = interrupt({\n",
    "            \"type\":\"query\",\n",
    "                \"question\": f\"Please provide data for {privious_state}\"\n",
    "            })\n",
    "    print(privious_state)\n",
    "    state[\"responce\"]=kt.get(\"status\")\n",
    "    return state\n",
    "\n",
    "\n",
    "initail_graph=StateGraph(InitailStateState)\n",
    "# initail_graph.add_node(\"inital_chat\",inital_chat)\n",
    "initail_graph.add_node(\"math\",math)\n",
    "initail_graph.add_node(\"knowledge\",knowledge)\n",
    "initail_graph.add_node(\"out_of_my_known\",out_of_my_known)\n",
    "initail_graph.add_node(\"ground\",ground)\n",
    "\n",
    "initail_graph.add_conditional_edges(START,inital_chat,{\n",
    "        \"math\": \"math\",\n",
    "        \"knowledge\": \"knowledge\",\n",
    "        \"out_of_my_known\":\"out_of_my_known\",\n",
    "        \"ground\":\"ground\"\n",
    "    })\n",
    "initail_graph.add_edge(\"math\",END)\n",
    "initail_graph.add_edge(\"knowledge\",END)\n",
    "initail_graph.add_edge(\"out_of_my_known\",END)\n",
    "initail_graph.add_edge(\"ground\",END)\n",
    "app=initail_graph.compile(checkpointer=checkpointer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'end time is 12', 'responce': 'out of know'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"query\":\"end time is 12\"},config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'i want to book ground',\n",
       " '__interrupt__': [Interrupt(value={'question': 'Please provide equation'}, id='e95ae29667a8c49871d6c52b1d7d5446')]}"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({\"query\":\"i want to book ground\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"responce\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=llm_with_tools.invoke(\"what is 3 +1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 73, 'total_tokens': 97, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_ee69c2ef48', 'id': 'chatcmpl-Co2JIKdZsNhcsSGRFZRis9pxxKaxv', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b3043-e815-7b02-ac66-d9c0acfca6f7-0', tool_calls=[{'name': 'math_tool', 'args': {'first_num': 3, 'second_num': 1, 'operation': 'add'}, 'id': 'call_iO1pBKFTKRF1SBJHvB52WL3a', 'type': 'tool_call'}], usage_metadata={'input_tokens': 73, 'output_tokens': 24, 'total_tokens': 97, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'date': '2025-12-20'}\n",
      "{'date': '2025-12-20'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# -----------------------------\n",
    "# BookingSchema definition\n",
    "# -----------------------------\n",
    "class BookingSchema(TypedDict, total=False):\n",
    "    user_id: str\n",
    "    start_time: str\n",
    "    end_time: str\n",
    "    date: str\n",
    "    status: str\n",
    "\n",
    "# -----------------------------\n",
    "# OpenAI API key setup\n",
    "# -----------------------------\n",
    "OPENAI_API_KEY=\"\"\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "# -----------------------------\n",
    "# Prompt template\n",
    "# -----------------------------\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a booking assistant. Respond ONLY with a JSON object following this schema: \n",
    "BookingSchema(TypedDict, total=False) with fields user_id, start_time, end_time, date, status. \n",
    "Include ONLY the fields mentioned or inferred from the user query. \n",
    "If some fields were previously provided in the state, keep them and update with new info from this query.\n",
    "Do NOT include extra text or explanation.\n",
    "\n",
    "Previous state: {previous_state}\n",
    "\n",
    "User query: \"{user_query}\"\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# Function to update single booking state\n",
    "# -----------------------------\n",
    "def update_booking_state(user_query: str, previous_state: BookingSchema) -> BookingSchema:\n",
    "    prev_state_str = json.dumps(previous_state)\n",
    "    prompt = PROMPT_TEMPLATE.format(user_query=user_query, previous_state=prev_state_str)\n",
    "\n",
    "    # New syntax for v1.0+ OpenAI Python SDK\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-5-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful booking assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "       \n",
    "    )\n",
    "\n",
    "    ai_text = response.choices[0].message.content.strip()\n",
    "\n",
    "    try:\n",
    "        updated_state = json.loads(ai_text)\n",
    "        \n",
    "        return updated_state\n",
    "    except json.JSONDecodeError:\n",
    "        return previous_state\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "def k():\n",
    "    state: BookingSchema = {}\n",
    "\n",
    "    user_input1 = \"Booking on 2025-12-20\"\n",
    "    state = update_booking_state(user_input1, state)\n",
    "    print(state)\n",
    "\n",
    "    # user_input2 = \"Start at 10:00\"\n",
    "    # state = update_booking_state(user_input2, state)\n",
    "    # print(state)\n",
    "\n",
    "    # user_input3 = \"User ID is U123\"\n",
    "    # state = update_booking_state(user_input3, state)\n",
    "    # print(state)\n",
    "\n",
    "k()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class ChatState(TypedDict):\n",
    "\n",
    "    query: str\n",
    "    responce:str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: ChatState):\n",
    "\n",
    "    decision = interrupt({\n",
    "        \"type\": \"responce\",\n",
    "        \"question\": \"my ground book quation\",\n",
    "        \"instruction\": \"Approve this question? yes/no\"\n",
    "    })\n",
    "    \n",
    "    if decision[\"responce\"] == 'no':\n",
    "        return {\"messages\": [AIMessage(content=\"Not approved.\")]}\n",
    "\n",
    "    else:\n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Build the graph: START -> chat -> END\n",
    "builder = StateGraph(ChatState)\n",
    "\n",
    "builder.add_node(\"chat\", chat_node)\n",
    "\n",
    "builder.add_edge(START, \"chat\")\n",
    "builder.add_edge(\"chat\", END)\n",
    "\n",
    "# Checkpointer is required for interrupts\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile the app\n",
    "app = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new thread id for this conversation\n",
    "config = {\"configurable\": {\"thread_id\": '1234'}}\n",
    "\n",
    "# ---- STEP 1: user asks a question ----\n",
    "initial_input = {\n",
    "    \"messages\": [\n",
    "        (\"user\", \"Explain gradient descent in very simple terms.\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Invoke the graph for the first time\n",
    "result = app.invoke(initial_input, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'responce',\n",
       " 'question': 'my ground book quation',\n",
       " 'instruction': 'Approve this question? yes/no'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = result['__interrupt__'][0].value\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume the graph with the approval decision\n",
    "final_result = app.invoke(\n",
    "    Command(resume={\"responce\": \"user_input\"}),\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Explain gradient descent in very simple terms.', additional_kwargs={}, response_metadata={}, id='879db7c7-1bd4-47ca-bdc4-20dd0c7d8105'),\n",
       "  AIMessage(content='Sure! Imagine you\\'re on a hill and your goal is to find the lowest point in the valley. You can\\'t see the whole valley, but you can feel the slope of the ground beneath your feet.\\n\\n1. **Start at a Point**: You begin at a random spot on the hill.\\n2. **Feel the Slope**: You check which direction is downhill (the steepest slope).\\n3. **Take a Step**: You take a small step in that downhill direction.\\n4. **Repeat**: You keep checking the slope and taking steps until you canâ€™t go any lower.\\n\\nIn the context of machine learning, gradient descent is a method used to minimize a function (like finding the best fit for a line in data). The \"hill\" represents the error or loss, and the \"lowest point\" is where the error is minimized. By repeatedly adjusting the parameters (like taking steps), we find the best solution.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 188, 'prompt_tokens': 15, 'total_tokens': 203, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-Co4dji5umfkdeHS8DBtVcmgNP7Q7T', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b30cc-9035-7400-8056-0a40d7de515e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 188, 'total_tokens': 203, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
